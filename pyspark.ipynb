{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17653984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125f82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1508fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/21 08:35:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession as SS\n",
    "spark=SS.builder.appName(\"WordCount\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d78fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file=spark.sparkContext.textFile(\"/home/labuser/Desktop/testfile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd21d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=text_file.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da0b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts=words.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c26ccdf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5c31a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count=word_counts.reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73ca05ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.parallelize([1,23,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c35102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 23, 4, 5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7de1c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list=['a','a','b','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f84e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddtest=spark.sparkContext.parallelize(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29ee1403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b', 'c']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddtest.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a2bc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=rddtest.flatMap(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e13ec11b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduceByKey' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lc\u001b[38;5;241m=\u001b[39mrddtest\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m words:(words,\u001b[38;5;241m1\u001b[39m),reduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m a,b:a\u001b[38;5;241m+\u001b[39mb))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reduceByKey' is not defined"
     ]
    }
   ],
   "source": [
    "lc=rddtest.map(lambda words:(words,1),reduceByKey(lambda a,b:a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d38a4161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this : 1\n",
      "sample : 1\n",
      "text : 1\n",
      "for : 1\n",
      "word : 1\n",
      "is : 1\n",
      "a : 1\n",
      "document : 1\n",
      "count : 1\n",
      "example : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sc=spark.sparkContext\n",
    "words_list = [\"this\" , \"is\" , \"a\" , \"sample\" , \"text\" , \"document\", \"for\" , \"word\" ,  \"count\" , \"example\"]\n",
    "rdd1 = sc.parallelize(words_list)\n",
    "\n",
    "res = rdd1.map(lambda x : (x,1))\n",
    "\n",
    "word_counts = rdd1.map(lambda x: (x,1)).reduceByKey(lambda a,b : a + b)\n",
    "\n",
    "results = word_counts.collect()\n",
    "\n",
    " \n",
    "\n",
    "for word,count in results:\n",
    "    print(f\"{word} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fccd305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|   _c0|   _c1|    _c2|\n",
      "+------+------+-------+\n",
      "|  null|apples|oranges|\n",
      "|  June|     3|      0|\n",
      "|Robert|     2|      3|\n",
      "|  Lily|     0|      7|\n",
      "| David|     1|      2|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/labuser/Documents/Pandas_datasets/IMDB-Movie_Data.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m purchasedf\u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/labuser/Documents/Pandas_datasets/purchases.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m purchasedf\u001b[38;5;241m.\u001b[39mshow() \u001b[38;5;66;03m#column issue , schema issue, data type issue\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m purchasedf_new\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/labuser/Documents/Pandas_datasets/IMDB-Movie_Data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m purchasedf_new\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:727\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/labuser/Documents/Pandas_datasets/IMDB-Movie_Data.csv."
     ]
    }
   ],
   "source": [
    "purchasedf= spark.read.csv(\"/home/labuser/Documents/Pandas_datasets/purchases.csv\")\n",
    "purchasedf.show() #column issue , schema issue, data type issue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6cf67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba9f0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "udfschema=StructType([StructField(\"Rank\",IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04adf1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Rank|\n",
      "+----+\n",
      "|   1|\n",
      "|   2|\n",
      "|   3|\n",
      "|   4|\n",
      "|   5|\n",
      "|   6|\n",
      "|   7|\n",
      "|   8|\n",
      "|   9|\n",
      "|  10|\n",
      "|  11|\n",
      "|  12|\n",
      "|  13|\n",
      "|  14|\n",
      "|  15|\n",
      "|  16|\n",
      "|  17|\n",
      "|  18|\n",
      "|  19|\n",
      "|  20|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/21 09:31:09 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 12, schema size: 1\n",
      "CSV file: file:///home/labuser/Documents/Pandas_datasets/IMDB-Movie-Data.csv\n"
     ]
    }
   ],
   "source": [
    "purchasedf_new=spark.read.schema(udfschema).option(\"header\",True).csv(\"/home/labuser/Documents/Pandas_datasets/IMDB-Movie-Data.csv\")\n",
    "purchasedf_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1db881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
